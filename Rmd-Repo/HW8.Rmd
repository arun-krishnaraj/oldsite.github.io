---
title: "HW 8"
author: "SDS348 Fall 2020"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
---

```{r global_options, include=FALSE}
#DO NOT EDIT THIS CHUNK OR ANYTHING ABOVE IT!
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F, tidy=T, tidy.opts=list(width.cutoff=50), R.options=list(max.print=100,dplyr.print_max=100))
library(tidyverse)
```

## Arun Krishnaraj - ak37738

**This homework is due Sunday Nov 1, 2020 at 11:59pm. Please submit as an HTML file on Canvas.**

*For all questions, include the R commands/functions that you used to find your answer. Answers without supporting code will not receive credit.*

> **Review of how to submit this assignment**
> All homework assignments will be completed using R Markdown. These `.Rmd` files consist of text/syntax (formatted using Markdown) alongside embedded R code. 
> When you have completed the assignment (by adding R code inside codeblocks and supporting text outside of the codeblocks), create your document as follows:

> - Click the arrow next to the "Knit" button (above) 
> - Choose "Knit to HTML" and wait; fix any errors if applicable
> - Go to Files pane and put checkmark next to the correct HTML file
> - Click on the blue gear icon ("More") and click Export
> - Download the file and then upload to Canvas


---

## Question 1 

We will analyze some data from a famous case of alleged gender discrimination in admission to graduate programs at UC Berkeley in 1973. The three variables are `Admit` (Admitted, Rejected), `Gender` (Male, Female), and `Dept` (Departments A, B, C, D, E, F). First, create a dichotomous outcome variable $y$ that is 1 if Admit=="Admitted" and 0 otherwise.

### 1.1 (2 pts) 

Predict $y$ from Gender using a logistic regression. Is the effect significant? Interpret the effect: what is the odds ratio for admission to graduate school for women compared to men? Compute the predicted probability of admission for women and for men.

```{R}
library(tidyverse)
adm <- read_csv("http://www.nathanielwoodward.com/admissions.csv")

adm$y <- as.numeric(adm$Admit =="Admitted")

model1 <- glm(y ~ Gender, data = adm, family = "binomial")
summary(model1)

exp(coef(model1))[1] -> a
exp(sum(coef(model1))) -> b

a/(1+a);b/(1+b)

```

The effect of gender on log odds of acceptance is significant; The odds ratio for admission to graduate school for women is `0.4358357`, while the odds ratio for admission for men is `0.8024065`.The predicted probability of admission is `.3035` for women and `.445`.

### 1.2 (2 pts) 

Now predict $y$ (admission) from Dept using a logistic regression. Interpret the overall pattern of results. For which departments are odds of admission higher than A? Which departments are the most selective? The least?

```{R}
model2 <- glm(y~Dept, data = adm, family = "binomial")
summary(model2)
```

Overall, it seems that Departments A and B are similar as far as selectivity, while the remaining 4 departments have significantly different odds of admission. All departments are more selective than Department A, with Department F being the most selective, and A being the least selective.

### 1.3 (2 pts) 

Rerun the model but add `Dept` (Department of graduate program) as a predictor. Interpret the coefficient for `Gender` now (note there is no interaction, so the effect doesn't depend on the level of `Dept`). Controlling for Department, is there a significant effect of Gender on admissions? What is the odds ratio? What can you say about departments A and B compared to the others (in terms of odds/probability of admission; relevel if need be)?

```{R}
model3 <- glm(y~Gender + Dept, data = adm, family = "binomial")
summary(model3)
```

Controlling for department, there doesn't seem to be a significant effect of gender on admissions; being male multiplies the odds of admission by `0.905`. It also seems like applying to Department A or B doesn't have a significant effect on odds of admissions.

### 1.4 (2 pts) 

OK, now add the interaction of Gender and Department as you predict $y$ (admissions), to get a fuller picture. Compute the odds ratio for admission (Male vs. Female) in each department (A through F). Which departments favor Male applicants (i.e., higher odds of admission for Males)?

```{R}
model4 <- glm(y~Gender * Dept, data = adm, family = "binomial")
summary(model4)
```

Department A has admission odds of 4.68 for females and 1.636 for males. Department B has admission odds of 2.13 for females and 1.71 for males. Department C has admission odds of .5166 for females and .585 for males. Department D has admission odds of .536 for females and .495 for males. Department E has admission odds of .314 for females and .384 for males. Department F has admission odds of .076 for females and .063 for males. Departments C and E favor male applicants.

### 1.5 (2 pts) 

Take the admit dataset and, using dplyr functions, create a table with counts of applicants of each Gender in each Department (e.g., number of males who applied to department A) and also the percent of applicants admitted of each Gender in each Department. Sort descending by the count variable. In terms of selectivity, what kinds of departments did the majority of women apply to? What about the majority of men? Skim through the wikipedia article about Simpson's paradox (https://en.wikipedia.org/wiki/Simpsons_paradox) to get a better idea of what is going on here!

```{R}
adm %>% group_by(Dept) %>% summarize(Male = sum(Gender == "Male"), Female = sum(Gender == "Female"), Male_admit = (sum(Gender == "Male" & Admit == "Admitted")/Male), Female_admit = (sum(Gender == "Female" & Admit == "Admitted"))/Female)
```

Women applied primarily to Department C, followed by Departments D, E, and F; men applied primarily to Departments A, B, and D. It seems like the departments which were found to favor males in admission have low number of male applicants, and that overall there is no bias across departments to admit men; women were found to tend to apply to more competitive departments than men, and correspondingly had lower admissions rates. 

## Question 2

Load the `starwars` data (from the dplyr package). Select just the variables `mass`, `height`, and `species` (these three variables only), remove all of the NAs from these, and save the result as `starwars1`.  Create a binary numeric variable $y$, $y=1$ if species is Human, $y=0$ otherwise, and add it as a column in `starwars1` (e.g., using mutate). Use this modified dataset (`starwars1`) for the remaining questions.


## 2.1 (3 pts) 

Predict the dichotomous Human indicator variable (`y`) that you just created from `height` using a logistic regression. Briefly interpret. Plot the ROC curve and compute the AUC (don't worry: it should be terrible). Create a plot of the logistic regression showing predicted probability of being Human by height. Color points by predicted human vs predicted not.

```{R}
starwars %>% select(mass, height, species) %>% filter(!is.na(mass) & !is.na(height) & !is.na(species)) -> starwars1
starwars1 %>% mutate(y = as.integer(species == "Human")) -> starwars1

glm(y~height, data = starwars1) -> model5
summary(model5)

library(plotROC)
ROCplot <- ggplot(model5) + geom_roc(aes(d=y,m=height), n.cuts = 0)
ROCplot
calc_auc(ROCplot)

starwars1$prob <- predict(model5, type = "response")
ggplot(starwars1, aes(x = height, y = prob)) + geom_point(aes(color = y))
```

Height is not able to significantly predict odds of a Star Wars character being human. The ROC curve is close to the 45-degree diagonal, indicating that the test is highly inaccurate. The AUC is .49, indicating that the model is less accurate in classification than random chance. 

## 2.2 (2 pts) 

Predict the Human indicator variable (`y`) from `height` and `mass` (no interaction). Discuss the output briefly (you do not have to interpret any coefficients). Compute Accuracy, Sensitivity, and Specificity. Plot the ROC curve and compute the AUC (it should still be bad)

```{R}
glm(y~height + mass, data = starwars1) -> model6
summary(model6)

ROCplot <- ggplot(model6) + geom_roc(aes(d=y,m=height), n.cuts = 0)
ROCplot
calc_auc(ROCplot)

prob <- predict(model6, type = "response")
pred <- ifelse(prob >= .4, 1, 0)
table(prediction = pred, truth = starwars1$y) %>% addmargins()
```

The model predicting human from height and mass without interaction produces insignificant results. The ROC is near the 45-diagonal, and the AUC of .49 indicates that the model is still performing worse than random. With a binary classification cutoff of .5, we have sensitivity of 0.41, specificity of 0.53, and accuracy of 0.48.

## 2.3 (3 pts) 

Predict the Human indicator variable (`y`) from the interaction of height and mass. Be sure to center your variables first, and save them to the `starwars1` dataset as `mass_c` and `height_c`. Discuss the output. Compute Accuracy, Sensitivity, and Specificity. Plot the ROC curve and calculate the AUC. Compare the AUC with that of the main-effects model in 2.2 (it should be a bit better).

```{R}
starwars1 %>% mutate(height_c = height - mean(height), mass_c = mass - mean(mass)) -> starwars1
glm(y~height_c * mass_c, data = starwars1) -> model7

summary(model7)

ROCplot <- ggplot(model7) + geom_roc(aes(d=y,m=height_c), n.cuts = 0)
ROCplot
calc_auc(ROCplot)

prob <- predict(model7, type = "response")
pred <- ifelse(prob >= .4, 1, 0)
table(prediction = pred, truth = starwars1$y) %>% addmargins()
```

This model of human status by centered height and weight produces a significant effect; the interaction term of height and weight is significant at the 5% level. The ROC is still near the 45-degree diagonal, but the AUC is slightly improved at 0.49. This model has sensitivity of .95, specificity of .44, and accuracy of 0.64.

## 2.4 (2 pts) 

We want to visualize the interaction, but it is continuous! We can get around this by setting mass_c to the mean (0) and plus/minus one standard deviation and then, for each of these three values, looking at the effect of height on the probability of being human. Below, in the code given, I take the dataset and I duplicate it three times: to one, I add a column with `mass_c=0`, to another, I add `mass_c=sd(mass)`, and to the third I add `mass_c=-sd(mass)`. Then, I stack them all on top of each other and add a label variable for each (`mass_cat`). Use this new dataset and `predict(your_fit_from_2.3, newdata=starwars_new, type="response")` to get predicted probabilities from your interaction model from 2.3, save the predicted probabilities in the dataset starwars_new, and then send the dataset to ggplot to plot those predicted probabilities (y-axis) against height (or height_c, on the x-axis) (use geom_line and set `color=mass_cat`). Interpret the interaction by describing what you see in the plot!

```{R}
## Code to get you started on 2d
starwars_new <- bind_rows(mutate(starwars1,mass_c=0), mutate(starwars1,mass_c=sd(mass)), mutate(starwars1,mass_c=-sd(mass)))

starwars_new <- starwars_new%>%
  mutate(mass_cat=c(rep("mean",nrow(starwars1)), rep("mean+1sd",nrow(starwars1)), rep("mean-1sd",nrow(starwars1))))

predict(model7, newdata = starwars_new, type = "response") -> starwars_new$pred

starwars_new %>% ggplot(aes(y = pred, x = height_c, color = mass_cat)) + geom_line()
```

The interaction effect between height and mass indicates that the slope effect of height on predicted odds of being human vary based on mass; higher mass corresponds to more negative slope, while lower mass corresponds to less negative slope. We can see that based on the mass category, height can have a range of effects on predicted human status. 


```{R, echo=F}
## DO NOT DELETE THIS BLOCK!
sessionInfo()
Sys.time()
Sys.info()
```