---
title: "M358K-Applied Statistics"
author: "ArunK"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[`r knitr::asis_output("\U1F332")`](https://arun-krishnaraj.github.io/evergreen)

### Notes
#### Lecture 9/2/2020
- Some introductory R programming review
- For every random variable $X$, its cumulative distribution function is the function $F_{x}: \mathbf{R} \rightarrow [0,1]$ given by $F_x (x) = \mathbf{P}[X \leq x]$ for all $x \space \epsilon \mathbf{R}$

#### 9/4/2020
- The simplest discrete random variable is the Bernoulli trial:
X~ 0 with probability 1-p(=q); 1 with probability p
-Its probability mass function is:
  $p_x(0) = \mathbb{P}[x=0]=1-p$,
  
  $p_x(1) = \mathbb{P}[x=1]=p$

- Note: this is an example of a named distribution with a single parameter $p$ interpreted as the probability of success in a single trial
- What happens if you repeat a number n = # of trials = size, of independent Bernoulli trials each with the same parameter $p$.
Let $X_1,...,X_n$ be independent Bernoulli(p). Then your total # of successes will be $S=X_1+X_2+...+X_n$, which results in Binomial(n,p). For the pmf: $p_S(k)= choose(n k) p^k(1-p)^{n-k})$ fro all k = 0...n.

#### Lecture 9/9/2020
- What is statistics?
  - Identify the broad population of interest about which you're trying to draw conclusions
  - What are we trying to measure?
  - The members of the population which you are trying to measure
    - cases (observational units) should become private labels 
    - variables can be quantitative (discrete or continuous) or qualitative (nominal or ordinal)
  - focus on figuring out the relative likelihoods of different outcomes
    - Probabilistic model:
      - named distribution with input parameters:
        - binomial(n,p); p is usually the parameter of interest 
        - geometric(p)
        - Poisson($\lambda$)
        - Normal(mean = $\mu$, variance = $\sigma^2$)
      - we are often interested in the behavior of non-named distributions 
  - collect and analyze data: find appropriate shape of distribtion and/or values of interest

#### Lecture 9/11/2020
- Using data sets pie-survey and titanic for some exploratory analysis

```{r}
pie <- read.csv("pie-survey-1.csv")
pielist <- table(pie$What.is.your.favorite.kind.of.pie.)
barplot(pielist)
pie(pielist)
```

#### Lecture 9/14/2020
```{r}
cafe <- read.csv("cafedata-1.csv")
plot(cafe$Wraps.Sold)
wraps <- cafe$Wraps.Sold[cafe$Wraps.Sold != "na"]
wraps <- as.numeric(wraps)
hist(wraps)
```

- sample mean, and other sample statistics, serve as point estimations of a true population statistic
  - data visualizations should be meaningful and accurate depictions; modality and skewedness
  - variance is a non-standardized measure of spread from the mean; weighs larger deviations more heavily, removes signs from deviation calculations

#### Lecture 9/16/2020
- median is the value that splits the data in half when ordered (mean of two medians if there are even observations)
  - IQR is the range between Q3 and Q1 (75th and 25th percentiles) and contains 50% of the sample data
    - add or subtract 1.5 times the IQR to Q3 or Q1 for whiskers 
  - more robust than mean to large outliers or deviations; good for highly skewed distributions
- highly skewed data can be transformed to be better (common log transform)
- exploratory base stats in the diamond dataset

#### Lecture 9/21/2020
- randomized response includes a randomized question and response to generate honest samples 
- observational studies: data is collected indirectly from the data arising 
  - prospective vs retrospective: identifies data prior or after events have taken place
- random sampling techniques:
  - simple: every sample is equally likely
  - stratified: split groups to better capture proportion
  - cluster: works if subsets of sample are alike 

#### Lecture 9/30/2020
- we use sample statistics as point estimates for the population parameter; we should also include a margin of error in the point estimate
- sample proportions will be normally distributed with mean p and standard error $= \sqrt{\frac{p(1-p)}{n}}$
  - require at least 10 expected successes and failures; independence of observations, and sample less than 10% of the population
  - becomes well approximated by a normal distribution $N(\mu, SE = \frac{\sigma}{\sqrt{n}})$

#### Lecture 10/02/2020
- continuous distributions: we can define the cdf of any random variable x as $F_x(x) = \mathbb{P}[X \leq x] \text{ for all } x \in \mathbb{R}$ 
  - if the cdf is:
    - continuous at all points
    - differentiable everywhere except at countably many points
  - then we can say that x is a continuous random variable 
- the function $f_x$ defined as $f_x(x) = F_{x}^{'}(x)$ wherever the derivative exists, is called the probability density function 
  - sufficient to provide the pdf to completely specify the distribution of a continuous random variable
  $$\mathbb{P}[a < X \leq b] = \mathbb{P}[X \leq b] - \mathbb{P}[x \leq a] = F_x(b) - F_x(a) = \int_{a}^{b} f_{x}(x)dx$$
- standard normal distribution: let the random variable Z have the density of this form $\phi(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}} \text{ for } z \in \mathbb{R}$
  - we then say that $z$ has the standard normal distribution
  - the cdf: $\Phi(x) = \int_{-\infty}^x \phi(z)dz = \int_{-\infty}^x\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}dz$
    - we don't have an analytic solution for this, so we use software or standard normal tables 
    - mean of $z$ is 0, and variance is 1
  - we use the parameterization of the mean and the variance to represent the normal distribution
  
#### Lecture 10/05/2020
- working through problem set during class 
- any normal random variable can be written as a linear transform of the standard normal distribution (conversion to Z scores)

#### Lecture 10/07/2020
- for any X and Y normally distributed random variables and any constants $\alpha$ and $\beta$, $\alpha X + \beta Y$ is a random variable which is also normally distributed 
  - linear combination of any normal random variables is normal
  - we can treat random samples as a set of n objects, which are independent and randomly distributed
    - define the sample mean as the average of observations, point estimator statistic
  $$\mathbb{E}[\bar{X}] = \mathbb{E}[\frac{1}{n}(X_1 + X_2 + ... +X_n)]= \frac{1}{n} (\mathbb{E}[X]*n) = \mathbb{E}[X]$$
  $$ Var[\bar{X}] = Var[\frac{1}{n}(X_1 + X_2 + ... +X_n)] = \frac{1}{n^2} Var[X_1 + X_2 + ... +X_n] = \frac{1}{n^2} Var[X]*n = \frac{Var[X]}{n}$$
  $$\rightarrow SD[\bar{X}] = \frac{SD[X]}{\sqrt{n}}$$
  - this gives us all the information we need to convert to Z-scores
  $\frac{\bar{X}- \mu_X}{\frac{\sigma_X}{\sqrt{n}}}$

#### Lecture 10/09/2020
- sample are iid random variables, $X \text{ ~ Normal(mean = }\mu_x \text{, sd = } \sigma_x) $
  - we want to be able to perform statistical inference even when exact parameters are unknown
  - converting $\bar{X}$ to Z-score results in the standard normal distribution
- confidence intervals
  - bounds set that contain a specified proportion $C$ of the distribution $\Phi^{-1}(\frac{1+C}{2})$

#### Lecture 10/12/2020
- each sample produces a different confidence interval, each of which contain the true parameter with probability C
- $\bar{x} \pm z^* \times \frac{\sigma_x}{\sqrt{n}}$, where the second term is the margin of error

#### Lecture 10/14/2020
- we use contradictions to refute null hypotheses 
  - alternative can be lower/upper tailed, or two-tailed

#### Lecture 10/16/2020
- we can calculate the probability of observing a given result under the null hypothesis
  - probability that standard normal falls more extreme than observed z-score = p-value
  - set $\alpha$ based on desired significance; generates lowest significance level for which the null hypothesis can be rejected
  - $\mu < \mu_o$ corresponds to $z = \frac{\bar{x} - \mu_o}{\sigma/\sqrt{n}} \leq z_\alpha = \Phi^{-1}(\alpha)$
  - $\mu > \mu_o$ corresponds to $z = \frac{\bar{x} - \mu_o}{\sigma/\sqrt{n}} \leq z{1-_\alpha} = \Phi^{-1}(\alpha)$

#### Lecture 10/19/2020
- p-value is the probability of seeing something as or more extreme under the null hypothesis; p-value is the lower bound of rejection 

#### Lecture 10/21/2020
- Type I and II errors occur when we reject the null and the null is actually true, or we  fail to reject the null and the null is actually false respectively
  - probability of a Type I error is the significance level $\alpha$
- probability of failing to reject is $\beta$; the power of the test is $1-\beta$
  
#### Lecture 10/23/2020
- power of the test approaches $\alpha$ as the alternative mean approaches the null mean
  - approaches 1 as the alternative mean diverges from the null mean
- power is the probability that the sample mean lands in the RR under the particular alternative

#### Lecture 10/26/2020
- we can calculate variance for binomial distribution, which can be used to construct confidence intervals $SD[X] = \sqrt{np \ (1-p)}$
  - $\lim_{n\to\infty} \frac{S_n - np}{\sqrt{np\ (1-p)}} = N(0,1)$
  - for large enough n ($np \ge 10$ and $n(1-p) \ge 10$), $S_n \approx N(mean = np, sd = \sqrt{np\ (1-p)})$ to generate counts
    - we can move from counts to proportion by $\hat{P}_n = \frac{S_n}{n} \approx N(mean = p, sd = \sqrt{\frac{p(1-p)}{n}})$

#### Lecture 10/28/2020
- we should use a continuity correction when using normal approximations for binomial to account for the change from discrete to continuous
- let $p$ denote the probability that a chosen member of the population has a certain discrete property
  - if we observe $X$ successes in a sample of size $n$, then $X \sim Binomial(n, p)$; $\hat{p}$ is the sample proportion or $\hat{p} = \frac{X}{n}$

#### Lecture 11/2/2020
- if $p$ denotes the probability of success in every trial (probability $p$ that a randomly chosen person has a trait of interest)
  - we can observe counts in the sample of size $n$: $X \sim Binomial(n,p)$; $\hat{p}$ is the sample of interest (sample proportion)
    - $X \sim Normal(mean =np,var= np(1-p))$, $\hat{p} \sim Normal(mean = p, var = \frac{p(1-p)}{n})$
    - we can then construct a confidence interval as $\hat{p} \pm z^* \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$ 
      - if we don't have $\hat{p}$ we can either use an estimate of a prior study, or use 1/2 to maximize variance of the Bernoulli trials
    
#### Lecture 11/4/2020
- for a large enough sample, $X \sim Normal(mean = np_o, var = np_o(1-p_o))$, leading to $\hat{P} \sim Normal(mean = p_o, var = \frac{p_o(1-p_o)}{n})$
  - the z-statistic is $z = \frac{\hat{p}-p_o}{\sqrt{\frac{p_o(1-p_o)}{n}}}$, which can then be used as cutoffs in the standard normal
- we can take parameters of interest $p_i$ for subpopulation $i$; assumption of independent samples 
  - if large enough, we can say that they are both normally distributed:
  $\hat{P_i} \sim Normal(mean = p_i, var = \frac{p_i(1-p_i)}{n_i})$
  - we are interested inthe difference between populations $\hat{P_1} -\hat{P_2} \sim Normal\big{(}mean = p_1-p_2, var = \frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}\big{)}$$

#### Lecture 11/6/2020
- in order to use the normal approximation to the binomial, we need independent samples and sufficient successes and failures
- we know that the proportion of two samples are normally distributed, so we can perform analysis of their difference
- we can construct confidence intervals of the form $\hat{p}_1 -\hat{p}_2 \pm z^*\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}$
- we can have one-sided or two sided alternative hypotheses when testing two parameters
  - the test statistic should be computed under the null hypothesis that $p_1 = p_2$
  
  $$\hat{P_1} - \hat{P_2} \sim Normal\bigg{(}mean = 0,\ var = \frac{p(1-p)}{n_1}+\frac{p(1-p)}{n_2}\bigg{)}$$
  $$\hat{P_1} - \hat{P_2} \sim Normal\bigg{(}mean = 0,\ var = p(1-p) \bigg{(}\frac{1}{n_1}+\frac{1}{n_2}\bigg{)}\bigg{)}$$
-where $\hat{p} = \frac{x_1+x_2}{n_1+n_2}$, pooled successes to obtain a reasonable estimate of the population proportion
  - we can use this normal approximation to convert observed differences to z-scores 

#### Lecture 11/9/2020
- practice using confidence intervals and hypothesis testing for two samples (refer to last class notes)

#### Lecture 11/11/2020
- $\chi^2$ random variable can be extended to include non-integers, but we will use:
  -If $Z_1, Z_2,...,Z_n$ are independent, standard normal random variables, then
$$X = Z_1^2+Z_2^2+...+Z_n^2$$ is $\chi^2$ distributed with n degrees of freedom
    - we write $Z\sim \chi^2$
  - the distribution shifts from right-skew to more normal with higher degrees of freedom
  - we can specify any value of r for the distribution, despite it being less nice to look at 
  - always interested in right-tailed events, $\alpha$; columns are $\alpha$ or left tail, and rows are degrees of freedom
  - since the distribution is continuous, we can use non-strict inequalities to calculate probabilities 
- if we have $X_1,X_2,...X_n$ iid normal variables, then for all $i = 1,...,n$, $Z_i = \frac{X_i - \mu}{\sigma}$ all $Z_i$ are independent and standard normal

$$Z_1^2+Z_2^2+...+Z_n^2 \sim \chi^2(df=n)$$
  - if the mean is unknown, we have instead 
  
  $$\sum_{i=1}^n\bigg(\frac{X_i-\bar{X}}{\sigma}\bigg)^2 \sim \chi^2(df=n-1)$$
$$\text{if we let } S^2 = \text{sample variance}$$
$$\rightarrow \frac{(n-1)\ S^2}{\sigma^2} \sim \chi^2(df=n-1)$$

#### Lecture 11/13/2020
- goodness of fit: looking at a multinomial experiment, with mutually exclusive and exhaustive outcomes
  - say that these categories are events $A_1,A_2,...,A_k$, where the parameters $p_i$ for $i=1...k$ which stand for $p_i = \mathbb{P}[A_i]$
  - if we repeat the same multinomial experiment $n$ times, we can measure $X_i$ for $i = 1..k$ as the number of times that outcome $i$ happened
  
  $$Q^2 = \sum_{i=1}^k\frac{(X_i - np_i)^2}{np_i} \sim \chi^2(df = k-1)$$
    - this works for $np_i \geq5$ for all $i=1...k$
  - the null hypothesis is that the true population probabilities are equal to null, and the alternative is that at least one population probability is different from the null 
  - using observed and expected notation, 
  
  $$q^2 = \sum_{i=1}^k\frac{(O_i - E_i)^2}{E_i} \sim \chi^2(df = k-1)$$
  - if our statistic is greater than the value for the given degrees of freedom and significance level, then we can reject the null hypothesis 

#### Lecture 11/16/2020
- we can use the chi-squared test of independence to determine dependence of two categorical treatments

$$X^2_{df} = \sum_{i=1}^k\frac{(O - E)^2}{E} \text{ where } df = (R-1)(C-1)$$
- use joint probability mass functions to compare to observed 
- letting $\mathbb{P}[X = x_i, Y=y_j] = p_{ij}$
  - if X and Y are independent, for all $i,j$
  
  $$p_{ij} = \mathbb{P}[X = x_i] \times \mathbb{P}[Y=y_j]$$

### Reading notes

Section 4.3 
- binomial distribution is used to describe the number of successes in a fixed number of trials 
  - describes the probability of exactly k successes in n independent Bernoulli trials with probability of success p
  $(n \text{ choose } k)\ p^k(1-p)^{n-k}, (n \text{ choose } k) = \frac{n!}{k!(n-k)!}$
- the normal distribution can be used to approximate the binomial distribution when the sample size is large
  - requires estimation of a large range of counts, or else performs poorly
  
Section 2.1.3
- plotting individual points can be bad for larger samples, instead bin and display counts in histogram
  - provide clear depiction of data density and distribution
  - skew occurs when tails occur in the direction of choice; roughly equal tails are symmetric 
  - mode: prominent peak in distribution, can have one, two or many modes
  
Section 1.2.1
- observations or cases appear as rows in data, with columns of variables

Section 1.2.2
- variables can be numeric, categorical, and continuous or discrete and nominal or ordinal

Section 2.2.1
- contingency tables summary count data for two categorical variables; individual groups, row, column and grand totals

Section 2.2.5
- pie charts present proportional information on categories

Section 2.1.2
- dot plot is a one variable scatterplot; the mean is the average, common way to measure the center of a distribution

Section 2.1.4
- deviance measures the distance from the mean, the mean of the squared deviation is the variance; sqrt of variance is sd

Section 2.1.5
- box plots summarize outliers, median, Q1 and Q3, upper and lower whiskers and outliers
- median is the observation splitting the data in 5 by counts; whiskers can only reach up to 1.5 times the IQR, never more

Section 2.1.6
- median and IQR are robust since extreme observations have little effect on them, compared to sd and mean

Section 1.2.3
- variables with some connection to each other are called associated, and can be called dependent
  - association can be positive or negative; otherwise they are independent
  
Section 1.3 Sampling principles and strategies
- research questions refer to individual target populations, of which a sample is taken to represent the average population member 
- anecdotal evidence is single cases used to draw general conclusions
- randomly selected samples are best
  - simple random sample is equivalent to raffle
  - non-response rate can influence sample composition
  - convenience sample: easily accessible people are more likely to be included in the sample 
- observational data occurs when no treatment is explicitly applied 
  - prospective studies identify individuals and collect data as it arises, compared to retrospective studies that collect data after events have taken place
- stratified sampling divides samples into similar cases then samples from each one
  - useful when stratum cases are similar in outcome of interest
- cluster sample has population broken into many clusters, with a fixed number of clusters being selected to generate the sample; multistage sampling can apply simple sampling to chosen clusters