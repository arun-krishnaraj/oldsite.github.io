---
title: "M358K - Project 2"
author: "Arun Krishnaraj - ak37738, Keven Li - kl32584, Lizbeth Rayas - ltr369,"
date: "10/16/2020"
output: pdf_document
---

```{r echo=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align="center",
  fig.pos="t",
  strip.white = TRUE
)
```

### Part One
#### Problem 1.1 
Brian Nosek undertook the replication project after reading an study which suggested the existence of ESP.
#### Problem 1.2 
The project consisted of independent replications of published psychology experiments. 
#### Problem 1.3 
It can be harmful to a scientist's career to repeat studies, since these results are time-consuming and less likely to be published or improve their academic reputation.
#### Problem 1.4 
The volunteer scientists repeated 100 experiments.
#### Problem 1.5 
The experiments were chosen from three top psychology journals; these experiments were not obscure within the field.
#### Problem 1.6 
The "afternoon-treat hypothesis" was that mental acuity would be improved after consuming a sugary food.
#### Problem 1.7
Brian Nosek waited until the entire experiment experiment was coompleted to view the results.
#### Problem 1.8
39 of the original conclusions were confirmed.
#### Problem 1.9
No, the conclusion is not that scientists are faking their data.
#### Problem 1.10
They conducted a coin-flipping experiment, and observed 9 heads in 10 flips of a fair coin.
#### Problem 1.11 
The file-drawer effect is the tendency of negative results to remain unpublished. It leads to interesting psychological phenomena being observed and published despite counter-evidence existing elsewhere, albeit unpublished.
#### Problem 1.12
The file-drawer effect doesn't completely explain the 39/100 ratio.
#### Problem 1.13
Dr. Lindsey describes the practice of adding more samples to existing data in an attempt to obtain a more accurate result.
#### Problem 1.14
Fields like economics, ecology, and cancer biology are doing the experiment experiment.
#### Problem 1.15
Brian Nosek proposes a registry system for experimental methods and results, regardless of outcome.
#### Problem 1.16
The registry system already exists in drug research; its implementation reduced the frequency of positive results from 50% to 8%.
#### Problem 1.17
We should not lose faith in scientific results, but we should be more careful in the way we interpret results and conduct experiments.

### Part Two

We are interested in whether R is able to generate tosses of a coin over 100 rolls; we should expect there to be an equal number of heads and tails returned
```{r}
library(ggplot2)

flip <- function(x) sample(c("H","T"), x, replace = T)

experiment <- function(n){
flips <- data.frame(index = 1:n, flip=flip(n))
for (i in 1:n) {
   flips$prop_h[i] <- (sum(flips[1:i,] == "H")/ flips$index[i])
   flips$prop_t[i] <- 1 - flips$prop_h[i]
}
return(flips)
}

set.seed(102938)
exp100 <- experiment(100)

ggplot(exp100, aes(x = index)) + geom_line(aes(y= prop_h), col = "dodgerblue") + xlab("Flip number") + ylab("Heads Proportion") + theme_minimal()
```

It seems like there are initially very "unfair" results to this experiment. The results seem to approach fairness around 30 tosses, but then remains somewhat unfair for the rest of the experiments. We end up with 61 heads out of 100 tosses, which would make it tempting to add more trials to get a more "fair" result. 

### Part Three
#### Problem 3.1
#### Problem 3.2
#### Problem 3.3